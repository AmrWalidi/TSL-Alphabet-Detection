{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T15:07:05.685392Z",
     "start_time": "2025-06-12T15:07:01.305923Z"
    }
   },
   "cell_type": "code",
   "source": "!nvidia-smi",
   "id": "76fd7e417ab8b5df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun 12 18:07:03 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 561.16                 Driver Version: 561.16         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3050 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   31C    P0              3W /   35W |       0MiB /   6144MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e60f2d5917842af",
   "metadata": {},
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "base_dir = \"dataset\"\n",
    "image_base = os.path.join(base_dir, \"images\")\n",
    "label_base = os.path.join(base_dir, \"labels\")"
   ],
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "splits = [\"train\", \"val\"]\n",
    "split_ratio = 0.8"
   ],
   "id": "36685629efacac57",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "all_images = [f for f in os.listdir(image_base) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "random.shuffle(all_images)\n",
    "\n",
    "split_index = int(len(all_images) * split_ratio)\n",
    "split_data = {\n",
    "    \"train\": all_images[:split_index],\n",
    "    \"val\": all_images[split_index:]\n",
    "}\n",
    "\n",
    "for split in splits:\n",
    "    split_image_folder = os.path.join(image_base, split)\n",
    "    os.makedirs(split_image_folder, exist_ok=True)\n",
    "    for image_file in split_data[split]:\n",
    "        src = os.path.join(image_base, image_file)\n",
    "        dst = os.path.join(split_image_folder, image_file)\n",
    "        shutil.move(src, dst)"
   ],
   "id": "ab50526912d87b7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image_size = 400\n",
    "\n",
    "base_path = f\"{base_dir}/images\"\n",
    "\n",
    "for split in splits:\n",
    "    folder_path = os.path.join(base_path, split)\n",
    "\n",
    "    for image_file in os.listdir(folder_path):\n",
    "        if not image_file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            continue\n",
    "\n",
    "        image_path = os.path.join(folder_path, image_file)\n",
    "        img = cv2.imread(image_path)\n",
    "\n",
    "        if img is None:\n",
    "            print(f\"❌ Couldn't read: {image_path}\")\n",
    "            continue\n",
    "\n",
    "        h, w = img.shape[:2]\n",
    "        img_white = np.ones((image_size, image_size, 3), np.uint8) * 255\n",
    "\n",
    "        aspect_ratio = h / w\n",
    "\n",
    "        try:\n",
    "            if aspect_ratio > 1:\n",
    "                scale = image_size / h\n",
    "                new_w = int(w * scale)\n",
    "                resized_img = cv2.resize(img, (new_w, image_size))\n",
    "                w_gap = (image_size - new_w) // 2\n",
    "                img_white[:, w_gap:w_gap + new_w] = resized_img\n",
    "            else:\n",
    "                scale = image_size / w\n",
    "                new_h = int(h * scale)\n",
    "                resized_img = cv2.resize(img, (image_size, new_h))\n",
    "                h_gap = (image_size - new_h) // 2\n",
    "                img_white[h_gap:h_gap + new_h, :] = resized_img\n",
    "\n",
    "            cv2.imwrite(image_path, img_white)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {image_path}: {e}\")\n"
   ],
   "id": "c4ee8432f7a6907d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class_names = [\n",
    "    'A', 'B', 'C', '+', 'D', 'E', 'F', 'G', ',', 'H', 'I', '!', 'J',\n",
    "    'K', 'L', 'M', 'N', 'O', '_', 'P', 'R', 'S', ';', 'T', 'U', '=',\n",
    "    'V', 'Y', 'Z'\n",
    "]\n",
    "class_to_id = {name: idx for idx, name in enumerate(class_names)}"
   ],
   "id": "5cc1efd43d057649"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for split in splits:\n",
    "    image_folder = os.path.join(image_base, split)\n",
    "    label_folder = os.path.join(label_base, split)\n",
    "    os.makedirs(label_folder, exist_ok=True)\n",
    "\n",
    "    for image_file in os.listdir(image_folder):\n",
    "        if image_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            try:\n",
    "                image_path = os.path.join(image_folder, image_file)\n",
    "                with Image.open(image_path) as img:\n",
    "                    width, height = img.size\n",
    "\n",
    "                class_name = image_file[0]\n",
    "                if class_name not in class_to_id:\n",
    "                    print(f\"❌ Skipping unknown class: {class_name}\")\n",
    "                    continue\n",
    "\n",
    "                class_id = class_to_id[class_name]\n",
    "                x_center = 0.5\n",
    "                y_center = 0.5\n",
    "                norm_width = 1.0\n",
    "                norm_height = 1.0\n",
    "\n",
    "                label_line = f\"{class_id} {x_center} {y_center} {norm_width} {norm_height}\\n\"\n",
    "                label_filename = os.path.splitext(image_file)[0] + \".txt\"\n",
    "                label_path = os.path.join(label_folder, label_filename)\n",
    "\n",
    "                with open(label_path, \"w\") as f:\n",
    "                    f.write(label_line)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed to process {image_file}: {e}\")\n"
   ],
   "id": "3b139018fbc8c524"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T16:22:51.145839Z",
     "start_time": "2025-06-12T16:22:51.107301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolov8n.pt')"
   ],
   "id": "ffa70ab8c94fcb16",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T17:20:38.180765Z",
     "start_time": "2025-06-12T17:02:15.297711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.train(\n",
    "    data='dataset.yaml',\n",
    "    epochs=20,\n",
    "    batch=16,\n",
    "    imgsz=400,\n",
    "    project='tsl_project',\n",
    "    name='tsl_yolo_train',\n",
    "    exist_ok=True\n",
    ")"
   ],
   "id": "cef4e66bca3f19ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.154 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.153  Python-3.12.8 torch-2.7.1+cpu CPU (13th Gen Intel Core(TM) i5-13420H)\n",
      "\u001B[34m\u001B[1mengine\\trainer: \u001B[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=dataset.yaml, degrees=5, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=20, erasing=0.4, exist_ok=True, fliplr=0.0, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.02, hsv_s=0.4, hsv_v=0.3, imgsz=400, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=0.5, multi_scale=False, name=tsl_yolo_train_2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.1, plots=True, pose=12.0, pretrained=True, profile=False, project=tsl_project, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=tsl_project\\tsl_yolo_train_2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.2, seed=0, shear=2, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    756967  ultralytics.nn.modules.head.Detect           [29, [64, 128, 256]]          \n",
      "Model summary: 129 layers, 3,016,503 parameters, 3,016,487 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "WARNING imgsz=[400] must be multiple of max stride 32, updating to [416]\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mFast image access  (ping: 0.00.0 ms, read: 735.9345.1 MB/s, size: 137.9 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mtrain: \u001B[0mScanning C:\\Users\\narut\\TSL-Alphabet-Detection\\model\\dataset\\labels\\train... 2377 images, 0 backgrounds, 0 corrupt: 100%|██████████| 2377/2377 [00:02<00:00, 973.76it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mtrain: \u001B[0mNew cache created: C:\\Users\\narut\\TSL-Alphabet-Detection\\model\\dataset\\labels\\train.cache\n",
      "\u001B[34m\u001B[1mval: \u001B[0mFast image access  (ping: 0.00.0 ms, read: 1117.6339.2 MB/s, size: 145.1 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\narut\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning C:\\Users\\narut\\TSL-Alphabet-Detection\\model\\dataset\\labels\\val... 595 images, 0 backgrounds, 0 corrupt: 100%|██████████| 595/595 [00:00<00:00, 1002.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mval: \u001B[0mNew cache created: C:\\Users\\narut\\TSL-Alphabet-Detection\\model\\dataset\\labels\\val.cache\n",
      "Plotting labels to tsl_project\\tsl_yolo_train_2\\labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\narut\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1moptimizer:\u001B[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001B[34m\u001B[1moptimizer:\u001B[0m AdamW(lr=0.000303, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 416 train, 416 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001B[1mtsl_project\\tsl_yolo_train_2\u001B[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/20         0G      1.553      6.217      2.032          5        416: 100%|██████████| 149/149 [05:26<00:00,  2.19s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:33<00:00,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        595        595    0.00753      0.891     0.0523     0.0408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/20         0G      1.166      5.368      1.638          6        416: 100%|██████████| 149/149 [05:16<00:00,  2.12s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:33<00:00,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        595        595      0.012      0.853     0.0754     0.0508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/20         0G      1.201      4.785      1.656          2        416: 100%|██████████| 149/149 [05:27<00:00,  2.20s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:33<00:00,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        595        595    0.00946       0.68     0.0324      0.018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/20         0G      1.084      4.603      1.577          4        416:   7%|▋         | 11/149 [00:25<05:25,  2.36s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      2\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mdataset.yaml\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m20\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m16\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43mimgsz\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m400\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhsv_h\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.02\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m           \u001B[49m\u001B[38;5;66;43;03m# small color changes\u001B[39;49;00m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhsv_s\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.4\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhsv_v\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtranslate\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# small shifts\u001B[39;49;00m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[43mscale\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m            \u001B[49m\u001B[38;5;66;43;03m# moderate scaling\u001B[39;49;00m\n\u001B[32m     11\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfliplr\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m           \u001B[49m\u001B[38;5;66;43;03m# flipping might break hand sign semantics\u001B[39;49;00m\n\u001B[32m     12\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmosaic\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m           \u001B[49m\u001B[38;5;66;43;03m# keep it low to moderate\u001B[39;49;00m\n\u001B[32m     13\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmixup\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m            \u001B[49m\u001B[38;5;66;43;03m# disable if your data is small or not diverse\u001B[39;49;00m\n\u001B[32m     14\u001B[39m \u001B[43m    \u001B[49m\u001B[43mperspective\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m      \u001B[49m\u001B[38;5;66;43;03m# slight perspective distortion\u001B[39;49;00m\n\u001B[32m     15\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdegrees\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m            \u001B[49m\u001B[38;5;66;43;03m# slight rotation\u001B[39;49;00m\n\u001B[32m     16\u001B[39m \u001B[43m    \u001B[49m\u001B[43mshear\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     17\u001B[39m \u001B[43m    \u001B[49m\u001B[43mproject\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mtsl_project\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     18\u001B[39m \u001B[43m    \u001B[49m\u001B[43mname\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mtsl_yolo_train_2\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     19\u001B[39m \u001B[43m    \u001B[49m\u001B[43mexist_ok\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[32m     20\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\model.py:797\u001B[39m, in \u001B[36mModel.train\u001B[39m\u001B[34m(self, trainer, **kwargs)\u001B[39m\n\u001B[32m    794\u001B[39m     \u001B[38;5;28mself\u001B[39m.model = \u001B[38;5;28mself\u001B[39m.trainer.model\n\u001B[32m    796\u001B[39m \u001B[38;5;28mself\u001B[39m.trainer.hub_session = \u001B[38;5;28mself\u001B[39m.session  \u001B[38;5;66;03m# attach optional HUB session\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m797\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    798\u001B[39m \u001B[38;5;66;03m# Update model and cfg after training\u001B[39;00m\n\u001B[32m    799\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m RANK \u001B[38;5;129;01min\u001B[39;00m {-\u001B[32m1\u001B[39m, \u001B[32m0\u001B[39m}:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:227\u001B[39m, in \u001B[36mBaseTrainer.train\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    224\u001B[39m         ddp_cleanup(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mstr\u001B[39m(file))\n\u001B[32m    226\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m227\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_do_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43mworld_size\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:415\u001B[39m, in \u001B[36mBaseTrainer._do_train\u001B[39m\u001B[34m(self, world_size)\u001B[39m\n\u001B[32m    410\u001B[39m     \u001B[38;5;28mself\u001B[39m.tloss = (\n\u001B[32m    411\u001B[39m         (\u001B[38;5;28mself\u001B[39m.tloss * i + \u001B[38;5;28mself\u001B[39m.loss_items) / (i + \u001B[32m1\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.tloss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.loss_items\n\u001B[32m    412\u001B[39m     )\n\u001B[32m    414\u001B[39m \u001B[38;5;66;03m# Backward\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m415\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mscaler\u001B[49m\u001B[43m.\u001B[49m\u001B[43mscale\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    417\u001B[39m \u001B[38;5;66;03m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001B[39;00m\n\u001B[32m    418\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m ni - last_opt_step >= \u001B[38;5;28mself\u001B[39m.accumulate:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:648\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    638\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    639\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    640\u001B[39m         Tensor.backward,\n\u001B[32m    641\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    646\u001B[39m         inputs=inputs,\n\u001B[32m    647\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m648\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    649\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    650\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    348\u001B[39m     retain_graph = create_graph\n\u001B[32m    350\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m353\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    356\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    357\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    822\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    823\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m824\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    825\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    826\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    827\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    828\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
